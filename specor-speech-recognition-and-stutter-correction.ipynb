{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7171840,"sourceType":"datasetVersion","datasetId":4143770},{"sourceId":7171843,"sourceType":"datasetVersion","datasetId":4143772},{"sourceId":7171864,"sourceType":"datasetVersion","datasetId":4143785},{"sourceId":7177548,"sourceType":"datasetVersion","datasetId":4148047},{"sourceId":7177667,"sourceType":"datasetVersion","datasetId":4148135},{"sourceId":7687455,"sourceType":"datasetVersion","datasetId":4486033},{"sourceId":7687555,"sourceType":"datasetVersion","datasetId":4486106},{"sourceId":7737409,"sourceType":"datasetVersion","datasetId":4522074},{"sourceId":7737443,"sourceType":"datasetVersion","datasetId":4522099},{"sourceId":7737452,"sourceType":"datasetVersion","datasetId":4522107},{"sourceId":165199443,"sourceType":"kernelVersion"}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n#! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n#import torch_xla\n# imports pytorch\n#import torch\n\n# imports the torch_xla package\n#import torch_xla\n#import torch_xla.core.xla_model as xm\n\n#device = xm.xla_device()\n#device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pydub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:22.311807Z","iopub.execute_input":"2024-03-02T00:03:22.312169Z","iopub.status.idle":"2024-03-02T00:03:22.607341Z","shell.execute_reply.started":"2024-03-02T00:03:22.312139Z","shell.execute_reply":"2024-03-02T00:03:22.606569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_csv_files(root_folder):\n    csv_count = 0\n\n    # Loop over all folders and files in the root_folder\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                csv_count += 1\n\n    return csv_count\n\n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\ncsv_count = count_csv_files(root_folder)\n\nif csv_count > 0:\n    print(f\"Number of .csv files found: {csv_count}\")\nelse:\n    print(\"No .csv files found.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:36.080989Z","iopub.execute_input":"2024-03-02T00:03:36.081456Z","iopub.status.idle":"2024-03-02T00:03:37.741642Z","shell.execute_reply.started":"2024-03-02T00:03:36.08141Z","shell.execute_reply":"2024-03-02T00:03:37.740745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def insert_audio_clip(background, audio_clip):\n    \"\"\"\n    Insert a new audio segment over the background noise at a random time step, ensuring that the\n    audio segment does not overlap with existing segments.\n\n    Arguments:\n    background -- a 10 second background audio recording.\n    audio_clip -- the audio clip to be inserted/overlaid.\n    previous_segments -- times where audio segments have already been placed\n\n    Returns:\n    new_background -- the updated background audio\n    \"\"\"\n    background -= 25\n    # Get the duration of the audio clip in ms\n    segment_ms = len(audio_clip)\n\n\n    new_background = background.overlay(audio_clip, position = 0)\n\n    return new_background","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:37.742641Z","iopub.execute_input":"2024-03-02T00:03:37.742932Z","iopub.status.idle":"2024-03-02T00:03:37.748404Z","shell.execute_reply.started":"2024-03-02T00:03:37.742907Z","shell.execute_reply":"2024-03-02T00:03:37.747553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Out_Noise_data = []","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:37.750948Z","iopub.execute_input":"2024-03-02T00:03:37.751284Z","iopub.status.idle":"2024-03-02T00:03:37.76463Z","shell.execute_reply.started":"2024-03-02T00:03:37.751254Z","shell.execute_reply":"2024-03-02T00:03:37.763842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydub import AudioSegment\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\ndef find_min_speech_length_and_path(root_folder,Out_Noise_data):\n    min_length = float('inf')  # Set initial value to positive infinity\n    min_path = None\n    \n    # Loop over all folders and files in the root_folder\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        for filename in filenames:\n\n            if filename.lower().endswith('.wav'):\n                file_path = os.path.join(folder_name, filename)\n                audio = AudioSegment.from_wav(file_path)\n                \n                Out_Noise_data.append(audio)\n\n                # Open the wav file using sf\n                with sf.SoundFile(file_path, 'r') as file:\n                    # Get the duration in seconds\n                    duration_seconds = len(file) / file.samplerate\n\n                    # Convert to milliseconds\n                    duration_ms = int(duration_seconds * 1000)\n\n                    # Update min_length and min_path if the current speech length is smaller\n                    if duration_ms < min_length:\n                        min_length = duration_ms\n                        min_path = file_path\n\n    return Out_Noise_data, min_length, min_path\n\n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/noise-data-set/noise_train'\nOut_Noise_data,min_speech_length, min_path = find_min_speech_length_and_path(root_folder,Out_Noise_data)\n\nif min_path:\n    print(f\"The minimum speech length is {min_speech_length} milliseconds, found in file: {min_path}\")\nelse:\n    print(\"No .wav files found.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:37.765911Z","iopub.execute_input":"2024-03-02T00:03:37.766263Z","iopub.status.idle":"2024-03-02T00:03:42.242395Z","shell.execute_reply.started":"2024-03-02T00:03:37.766233Z","shell.execute_reply":"2024-03-02T00:03:42.241433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(Out_Noise_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:42.243521Z","iopub.execute_input":"2024-03-02T00:03:42.24383Z","iopub.status.idle":"2024-03-02T00:03:42.251013Z","shell.execute_reply.started":"2024-03-02T00:03:42.243804Z","shell.execute_reply":"2024-03-02T00:03:42.250095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\ndef find_min_speech_length_and_path(root_folder,Out_Noise_data):\n    min_length = float('inf')  # Set initial value to positive infinity\n    min_path = None\n    \n    # Loop over all folders and files in the root_folder\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        for filename in filenames:\n\n            if filename.lower().endswith('.wav'):\n                file_path = os.path.join(folder_name, filename)\n                audio = AudioSegment.from_wav(file_path)\n                Out_Noise_data.append(audio)\n\n                # Open the wav file using sf\n                with sf.SoundFile(file_path, 'r') as file:\n                    # Get the duration in seconds\n                    duration_seconds = len(file) / file.samplerate\n\n                    # Convert to milliseconds\n                    duration_ms = int(duration_seconds * 1000)\n\n                    # Update min_length and min_path if the current speech length is smaller\n                    if duration_ms < min_length:\n                        min_length = duration_ms\n                        min_path = file_path\n\n    return Out_Noise_data, min_length, min_path\n\n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/noise-data-set/noise_test'\nOut_Noise_data,min_speech_length, min_path = find_min_speech_length_and_path(root_folder,Out_Noise_data)\n\nif min_path:\n    print(f\"The minimum speech length is {min_speech_length} milliseconds, found in file: {min_path}\")\nelse:\n    print(\"No .wav files found.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:42.252541Z","iopub.execute_input":"2024-03-02T00:03:42.25289Z","iopub.status.idle":"2024-03-02T00:03:43.084479Z","shell.execute_reply.started":"2024-03-02T00:03:42.252859Z","shell.execute_reply":"2024-03-02T00:03:43.083502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(Out_Noise_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:43.085925Z","iopub.execute_input":"2024-03-02T00:03:43.086341Z","iopub.status.idle":"2024-03-02T00:03:43.092575Z","shell.execute_reply.started":"2024-03-02T00:03:43.086308Z","shell.execute_reply":"2024-03-02T00:03:43.091647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport soundfile as sf\nimport random\nimport librosa\nimport numpy as np\nAudioSegment.converter = \"/path/to/ffmpeg\"\ndef find_max_speech_length(root_folder, Destination, Out_Noise_data):\n    max_length = 0\n\n    # Loop over all folders and files in the root_folder\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        for filename in filenames:\n            if filename.lower().endswith('.flac'):\n                file_path = os.path.join(folder_name, filename)\n\n                # Replace 'output.wav' with the desired path and name for the output WAV file\n                output_file = \"/kaggle/working/sample.wav\"\n                # Open the FLAC file and convert it to WAV\n                # Read the FLAC audio file             \n                data, current_sample_rate = sf.read(file_path)\n                 \n                #break\n                sf.write(output_file, data, current_sample_rate)\n\n                AudioSegment.converter = \"/path/to/ffmpeg\"\n                Audio = AudioSegment.from_wav(output_file)\n                Noise =  Out_Noise_data[random.randrange(0, 179)]\n                audio_clip= None\n                if(len(Noise) > len(Audio)):\n                    audio_clip = insert_audio_clip(Noise[:len(Audio)], Audio)\n                else:\n                    repeat_factor = int(np.ceil((len(Audio)+0.0) / len(Noise)))\n                    # Repeat the noise\n                    RepeatedNoise = Noise * repeat_factor\n                    # Trim the repeated noise to match the length of the audio\n                    audio_clip = insert_audio_clip(RepeatedNoise[:len(Audio)], Audio)\n\n                output_path = os.path.join(Destination, file_path[40:-5] + '.wav')\n                os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create directories if they don't exist\n                audio_clip.export(output_path, format=\"wav\")\n\n\n\n    return max_length\n\n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Audio'\nDestination = '/kaggle/working/LibriStutter_Data/'\nmax_speech_length = find_max_speech_length(root_folder, Destination, Out_Noise_data)\n\n\nprint(f\"The maximum speech length is {max_speech_length} milliseconds.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:03:43.093702Z","iopub.execute_input":"2024-03-02T00:03:43.094014Z","iopub.status.idle":"2024-03-02T00:06:54.914024Z","shell.execute_reply.started":"2024-03-02T00:03:43.09399Z","shell.execute_reply":"2024-03-02T00:06:54.913009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tiktoken\n!pip install more_itertools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom functools import lru_cache\nfrom subprocess import CalledProcessError, run\nfrom typing import Optional, Union\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport os\nos.chdir(\"/kaggle/input/feature-extraction\")\nfrom audio import (FRAMES_PER_SECOND,HOP_LENGTH,\n    N_FRAMES,N_SAMPLES,SAMPLE_RATE, CHUNK_LENGTH, FRAMES_PER_SECOND,\n    load_audio, exact_div, pad_or_trim, mel_filters, log_mel_spectrogram)\nos.chdir(\"/kaggle/input/filesa-data-speech/\")\nfrom english import EnglishTextNormalizer\nfrom english import (EnglishNumberNormalizer,EnglishSpellingNormalizer,)\nfrom basic import BasicTextNormalizer as BasicTextNormalizer\n#!pip install  ffmpeg","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:06:54.917682Z","iopub.execute_input":"2024-03-02T00:06:54.918033Z","iopub.status.idle":"2024-03-02T00:06:54.922708Z","shell.execute_reply.started":"2024-03-02T00:06:54.918009Z","shell.execute_reply":"2024-03-02T00:06:54.921792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Norm11=BasicTextNormalizer()\nNorm12=EnglishTextNormalizer()\nNorm13=EnglishSpellingNormalizer()\nNorm14=EnglishNumberNormalizer()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:06:54.923839Z","iopub.execute_input":"2024-03-02T00:06:54.924097Z","iopub.status.idle":"2024-03-02T00:06:54.948115Z","shell.execute_reply.started":"2024-03-02T00:06:54.924075Z","shell.execute_reply":"2024-03-02T00:06:54.947433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!apt update -y && apt upgrade -y\n!apt install sox libsox-fmt-mp3 -y\n!apt install ffmpeg -y","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:07:19.284115Z","iopub.execute_input":"2024-03-02T00:07:19.284392Z","iopub.status.idle":"2024-03-02T00:07:43.677944Z","shell.execute_reply.started":"2024-03-02T00:07:19.284359Z","shell.execute_reply":"2024-03-02T00:07:43.676766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import os\nimport pandas as pd\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\nL_Audio=[]\nL_Txt=[]\n\ndef find_min_speech_length_and_path(root_folder):\n    # Loop over all folders and files in the root_folder\n    io = 0\n    exi = False\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        print(folder_name)\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n                io+=1\n                #print(file_path)\n                L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n                file_path = os.path.join(folder_name, filename)\n                df = pd.read_csv(file_path,header =None)\n                formatted_line=\"\"\n                for _, row in df.iterrows():\n                    text = row[0]\n                    start_time = row[1]\n                    end_time = row[2]\n                    \n                    if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n                        formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                    else:\n                        formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n                    #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                #with open(file_path, 'r') as file:\n                #    file_contents = file.read()\n                L_Txt.append(formatted_line)\n                if(io == 1000):\n                    exi = True\n                    break\n        if(exi):\n            break\n    \n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\nfind_min_speech_length_and_path(root_folder)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:07:43.679465Z","iopub.execute_input":"2024-03-02T00:07:43.679797Z","iopub.status.idle":"2024-03-02T00:08:09.162813Z","shell.execute_reply.started":"2024-03-02T00:07:43.679756Z","shell.execute_reply":"2024-03-02T00:08:09.162054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import os\nimport pandas as pd\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\nL_Audio=[]\nL_Txt=[]\n\ndef find_min_speech_length_and_path(root_folder):\n    # Loop over all folders and files in the root_folder\n    io = 0\n    \n    exi = False\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        print(folder_name)\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n                io+=1\n                if(io > 1000 ):\n                    #print(file_path)\n                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n                    file_path = os.path.join(folder_name, filename)\n                    df = pd.read_csv(file_path,header =None)\n                    formatted_line=\"\"\n                    for _, row in df.iterrows():\n                        text = row[0]\n                        start_time = row[1]\n                        end_time = row[2]\n\n                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                        else:\n                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                    #with open(file_path, 'r') as file:\n                    #    file_contents = file.read()\n                    L_Txt.append(formatted_line)\n                if(io == 2000):\n                    exi = True\n                    break\n        if(exi):\n            break\n    \n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\nfind_min_speech_length_and_path(root_folder)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:09.164214Z","iopub.execute_input":"2024-03-02T00:08:09.164574Z","iopub.status.idle":"2024-03-02T00:08:09.17341Z","shell.execute_reply.started":"2024-03-02T00:08:09.164541Z","shell.execute_reply":"2024-03-02T00:08:09.172428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import os\nimport pandas as pd\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\nL_Audio=[]\nL_Txt=[]\n\ndef find_min_speech_length_and_path(root_folder):\n    # Loop over all folders and files in the root_folder\n    io = 0\n    \n    exi = False\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        print(folder_name)\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n                io+=1\n                if(io > 2000 ):\n                    #print(file_path)\n                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n                    file_path = os.path.join(folder_name, filename)\n                    df = pd.read_csv(file_path,header =None)\n                    formatted_line=\"\"\n                    for _, row in df.iterrows():\n                        text = row[0]\n                        start_time = row[1]\n                        end_time = row[2]\n\n                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                        else:\n                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                    #with open(file_path, 'r') as file:\n                    #    file_contents = file.read()\n                    L_Txt.append(formatted_line)\n                if(io == 3000):\n                    exi = True\n                    break\n        if(exi):\n            break\n    \n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\nfind_min_speech_length_and_path(root_folder)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:09.174587Z","iopub.execute_input":"2024-03-02T00:08:09.174872Z","iopub.status.idle":"2024-03-02T00:08:09.193062Z","shell.execute_reply.started":"2024-03-02T00:08:09.174849Z","shell.execute_reply":"2024-03-02T00:08:09.191853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\nL_Audio=[]\nL_Txt=[]\n\ndef find_min_speech_length_and_path(root_folder):\n    # Loop over all folders and files in the root_folder\n    io = 0\n    \n    exi = False\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        print(folder_name)\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n                io+=1\n                if(io > 3000 ):\n                    #print(file_path)\n                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n                    file_path = os.path.join(folder_name, filename)\n                    df = pd.read_csv(file_path,header =None)\n                    formatted_line=\"\"\n                    for _, row in df.iterrows():\n                        text = row[0]\n                        start_time = row[1]\n                        end_time = row[2]\n\n                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                        else:\n                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                    #with open(file_path, 'r') as file:\n                    #    file_contents = file.read()\n                    L_Txt.append(formatted_line)\n                if(io == 4000):\n                    exi = True\n                    break\n        if(exi):\n            break\n    \n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\nfind_min_speech_length_and_path(root_folder)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:09.194901Z","iopub.execute_input":"2024-03-02T00:08:09.195264Z","iopub.status.idle":"2024-03-02T00:08:09.209367Z","shell.execute_reply.started":"2024-03-02T00:08:09.195233Z","shell.execute_reply":"2024-03-02T00:08:09.208353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport soundfile as sf\nAudioSegment.converter = \"/path/to/ffmpeg\"\nL_Audio_val=[]\nL_Txt_val=[]\n\ndef find_min_speech_length_and_path(root_folder):\n    # Loop over all folders and files in the root_folder\n    io = 0\n    \n    exi = False\n    for folder_name, subfolders, filenames in os.walk(root_folder):\n        print(folder_name)\n        for filename in filenames:\n            if filename.lower().endswith('.csv'):\n                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n                io+=1\n                if(io > 4000 ):\n                    #print(file_path)\n                    L_Audio_val.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n                    file_path = os.path.join(folder_name, filename)\n                    df = pd.read_csv(file_path,header =None)\n                    formatted_line=\"\"\n                    for _, row in df.iterrows():\n                        text = row[0]\n                        start_time = row[1]\n                        end_time = row[2]\n\n                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                        else:\n                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n                    #with open(file_path, 'r') as file:\n                    #    file_contents = file.read()\n                    L_Txt_val.append(formatted_line)\n                if(io == 4500):\n                    exi = True\n                    break\n        if(exi):\n            break\n    \n# Replace '/path/to/root/folder' with the actual path to your root folder\nroot_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\nfind_min_speech_length_and_path(root_folder)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:09.210595Z","iopub.execute_input":"2024-03-02T00:08:09.210921Z","iopub.status.idle":"2024-03-02T00:08:25.731507Z","shell.execute_reply.started":"2024-03-02T00:08:09.210895Z","shell.execute_reply":"2024-03-02T00:08:25.730081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('len(L_Audio) -> ', len(L_Audio))\nprint('len(L_Audio_val) -> ',len(L_Audio_val))","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:25.732919Z","iopub.execute_input":"2024-03-02T00:08:25.733299Z","iopub.status.idle":"2024-03-02T00:08:25.739836Z","shell.execute_reply.started":"2024-03-02T00:08:25.733268Z","shell.execute_reply":"2024-03-02T00:08:25.73875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:25.740874Z","iopub.execute_input":"2024-03-02T00:08:25.741135Z","iopub.status.idle":"2024-03-02T00:08:25.754702Z","shell.execute_reply.started":"2024-03-02T00:08:25.74111Z","shell.execute_reply":"2024-03-02T00:08:25.753796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exact_div(x, y):\n    assert x % y == 0\n    return x // y\n\n\n# hard-coded audio hyperparameters\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nCHUNK_LENGTH = 30 # 30-second\nN_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\nN_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\nN_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\nFRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\nTOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:25.755907Z","iopub.execute_input":"2024-03-02T00:08:25.756232Z","iopub.status.idle":"2024-03-02T00:08:25.765609Z","shell.execute_reply.started":"2024-03-02T00:08:25.756201Z","shell.execute_reply":"2024-03-02T00:08:25.764814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tiktoken\n!pip install more_itertools\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:25.766681Z","iopub.execute_input":"2024-03-02T00:08:25.767252Z","iopub.status.idle":"2024-03-02T00:08:49.427773Z","shell.execute_reply.started":"2024-03-02T00:08:25.767228Z","shell.execute_reply":"2024-03-02T00:08:49.42684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/tokenizer\")\nfrom tokenizer import Tokenizer, get_tokenizer\nos.chdir(\"/kaggle/input\")\ntokenizer =  get_tokenizer()\nfrom torch.utils.data import DataLoader, Dataset\nfrom typing import  List, Optional, Tuple, Union\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:49.429222Z","iopub.execute_input":"2024-03-02T00:08:49.429522Z","iopub.status.idle":"2024-03-02T00:08:49.62703Z","shell.execute_reply.started":"2024-03-02T00:08:49.429495Z","shell.execute_reply":"2024-03-02T00:08:49.626246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.eot","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:49.628356Z","iopub.execute_input":"2024-03-02T00:08:49.628749Z","iopub.status.idle":"2024-03-02T00:08:49.63541Z","shell.execute_reply.started":"2024-03-02T00:08:49.628716Z","shell.execute_reply":"2024-03-02T00:08:49.634507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(\n        self,\n        mels,\n        records,\n        tokenizer: Tokenizer,\n        fp16: bool = True,\n        no_timestamps_training: bool = False,\n        max_prompt_length: int = 223,  # The maximum number of tokens to use for the prompt\n        prompt_use_rate: float = 0.5,\n        no_timestamps_rate: float = 0.5,\n    ) -> None:\n        self.mels = mels\n        self.records = records\n        self.tokenizer = tokenizer\n        self.fp16 = fp16\n        self.no_timestamps_training = no_timestamps_training\n        self.max_prompt_length = max_prompt_length\n        self.prompt_use_rate = prompt_use_rate\n        self.no_timestamps_rate = no_timestamps_rate\n\n        self.num_frames_per_second = N_FRAMES / CHUNK_LENGTH\n        # timestamps tokens are from <|0.00|> to <|30.00|> with a step of 0.02\n        self.timestamp_pattern = re.compile(r\"(<\\|[123]?[0-9]\\.[0-9][0-9]\\|>)\")\n        self.model_n_text_ctx = 448\n\n    def __len__(self) -> int:\n        return len(self.records)\n\n    def _get_prompt_tokens(self, prompt: str) -> List[int]:\n        if len(prompt) > 0 and torch.rand(1) < self.prompt_use_rate:\n            prompt_tokens = self._encode_text_with_timestamps(prompt)[-self.max_prompt_length :]\n            prompt_tokens = [self.tokenizer.sot_prev] + prompt_tokens\n        else:\n            prompt_tokens = []\n\n        return prompt_tokens\n\n    def _get_special_tokens(\n        self, is_text_empty: bool, language: str, no_timestamps: bool\n    ) -> List[int]:\n        if is_text_empty:\n            special_tokens = [self.tokenizer.sot, self.tokenizer.no_speech]\n        else:\n            special_tokens = [\n                self.tokenizer.sot,\n                self.tokenizer.special_tokens[f\"<|{language}|>\"],\n                self.tokenizer.special_tokens[\"<|transcribe|>\"],\n            ]\n            if no_timestamps:\n                special_tokens.append(self.tokenizer.no_timestamps)\n\n        return special_tokens\n\n    def _encode_text_with_timestamps(self, text: str) -> List[int]:\n        parts = self.timestamp_pattern.split(text)\n        parts = [token for token in parts if token != \"\"]\n        tokens = []\n        for part in parts:\n            if self.timestamp_pattern.fullmatch(part) is not None:\n                timestamp = float(part[2:-2])\n\n                # timestamp must be in the range [0, 30] and be a multiple of 0.02 seconds\n                if timestamp < 0 or timestamp > 30 or round(timestamp * 100) % 2 != 0:\n                    raise ValueError(f\"Invalid timestamp: {timestamp}\")\n\n                token = self.tokenizer.timestamp_begin + round(timestamp * 100) // 2\n                tokens.append(token)\n            else:\n                tokens.extend(self.tokenizer.encode(part))\n\n        return tokens\n\n    def _get_partial_segment_start(self, tokens: List[int]) -> Optional[float]:\n        if (\n            len(tokens) >= 2\n            and tokens[-2] >= self.tokenizer.timestamp_begin\n            and tokens[-1] >= self.tokenizer.timestamp_begin\n        ):  # if the last token is a start time token\n            return (tokens[-1] - self.tokenizer.timestamp_begin) * 0.02\n        else:\n            return None\n\n    def _get_text_tokens(self, text: str, no_timestamps: bool) -> Tuple[List[int], Optional[float]]:\n        text_tokens = self._encode_text_with_timestamps(text)\n        next_partial_segment_start = self._get_partial_segment_start(text_tokens)\n        if no_timestamps:\n            text_tokens = list(filter(lambda x: x < self.tokenizer.timestamp_begin, text_tokens))\n\n        return text_tokens, next_partial_segment_start\n\n    def _calculate_mel(\n        self, audio_path: str, next_partial_segment_start: Optional[float], no_timestamps: bool\n    ) -> torch.Tensor:\n        mel = log_mel_spectrogram(audio_path)\n        if no_timestamps and next_partial_segment_start is not None:\n            mel = mel[:, : int(next_partial_segment_start * self.num_frames_per_second)]\n        mel = pad_or_trim(mel, N_FRAMES)\n        if self.fp16:\n            mel = mel.half()\n\n        return mel\n\n    def _construct_decoder_output(\n        self, prompt_tokens: List[int], special_tokens: List[int], text_tokens: List[int]\n    ) -> List[int]:\n        if len(prompt_tokens) == 0:\n            decoder_output = special_tokens[1:] + text_tokens + [self.tokenizer.eot]\n        else:\n            decoder_output = (\n                # Mask out the training loss for predicting the prompt tokens. We use \"-100\" as the\n                # default value for the `ignore_index` parameter in\n                # `torch.nn.functional.cross_entropy()`. However, we do not mask out the loss for\n                # predicting the sot token because our experiment indicates that the original\n                # Whisper model assigns a high probability to the sot token after prompt tokens.\n                [-100] * (len(prompt_tokens) - 1)\n                + special_tokens\n                + text_tokens\n                + [self.tokenizer.eot]\n            )\n        return decoder_output\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        record = self.records[index]\n        mel = self.mels[index]\n        no_timestamps = self.no_timestamps_training\n        #no_timestamps = True\n        #print(no_timestamps)\n        text_tokens, next_partial_segment_start = self._get_text_tokens(record, no_timestamps)\n        is_text_empty = len(text_tokens) == 0\n        special_tokens = self._get_special_tokens(is_text_empty, \"en\", no_timestamps)\n\n        decoder_input = special_tokens + text_tokens\n        if len(decoder_input) > self.model_n_text_ctx:\n            raise ValueError(f\"Input is too long: {record} (length: {len(decoder_input)})\")\n\n        decoder_output = self._construct_decoder_output([],special_tokens, text_tokens)\n\n        \n        return (\n            mel,\n            torch.tensor(decoder_input, dtype=torch.long),\n            torch.tensor(decoder_output, dtype=torch.long),\n        )","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:08:49.636908Z","iopub.execute_input":"2024-03-02T00:08:49.637176Z","iopub.status.idle":"2024-03-02T00:08:49.664142Z","shell.execute_reply.started":"2024-03-02T00:08:49.637149Z","shell.execute_reply":"2024-03-02T00:08:49.663174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\ndef collate_fn(data):\n    x, y_in, y_out = zip(*data)\n    x = pad_sequence(x, batch_first=True, padding_value=0)\n    y_in = pad_sequence(y_in, batch_first=True, padding_value=50256)#eot token\n    y_out = pad_sequence(y_out, batch_first=True, padding_value=-100)\n    return x, y_in, y_out\ntokenizer =  get_tokenizer()\ndataset = AudioDataset(\n    L_Audio,\n    L_Txt,\n    tokenizer,\n    no_timestamps_training=False)\nDataLoaders = DataLoader(\n        dataset,\n        batch_size=1,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )\n\ndataset_val = AudioDataset(\n    L_Audio_val,\n    L_Txt_val,\n    tokenizer,\n    no_timestamps_training=False)\nDataLoaders_val = DataLoader(\n        dataset_val,\n        batch_size=4,\n        shuffle=True,\n        num_workers=4,\n        pin_memory=True,\n        collate_fn=collate_fn,\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:19.392217Z","iopub.execute_input":"2024-03-02T00:18:19.392644Z","iopub.status.idle":"2024-03-02T00:18:19.403688Z","shell.execute_reply.started":"2024-03-02T00:18:19.392605Z","shell.execute_reply":"2024-03-02T00:18:19.402628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nos.chdir(\"/kaggle/input/whispersa\")\nfrom model import (Whispersa,TextDecoder,AudioEncoder,ModelDimensions)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:19.726349Z","iopub.execute_input":"2024-03-02T00:18:19.727181Z","iopub.status.idle":"2024-03-02T00:18:19.73238Z","shell.execute_reply.started":"2024-03-02T00:18:19.727148Z","shell.execute_reply":"2024-03-02T00:18:19.731489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:21.850577Z","iopub.execute_input":"2024-03-02T00:18:21.851304Z","iopub.status.idle":"2024-03-02T00:18:21.855876Z","shell.execute_reply.started":"2024-03-02T00:18:21.851271Z","shell.execute_reply":"2024-03-02T00:18:21.855002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of ModelDimensions\ndimensions_instance = ModelDimensions(\n    n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=tokenizer.encoding.n_vocab, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n   # n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=51864, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n  \n)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:21.876443Z","iopub.execute_input":"2024-03-02T00:18:21.876717Z","iopub.status.idle":"2024-03-02T00:18:21.881755Z","shell.execute_reply.started":"2024-03-02T00:18:21.876693Z","shell.execute_reply":"2024-03-02T00:18:21.880743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n!pip install git+https://github.com/openai/whisper.git\nimport whisper\nmodela = whisper.load_model('small.en').to(device)\nmodel = Whispersa(dimensions_instance).to(device)\nmodel.load_state_dict(modela.state_dict())\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:21.928753Z","iopub.execute_input":"2024-03-02T00:18:21.929294Z","iopub.status.idle":"2024-03-02T00:18:54.48606Z","shell.execute_reply.started":"2024-03-02T00:18:21.929269Z","shell.execute_reply":"2024-03-02T00:18:54.484826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nmodel = Whispersa(dimensions_instance).to(device)\nsave_path = \"/kaggle/input/speech-pro1/model2.pt\"  # Replace with your desired path\nmodel.load_state_dict(torch.load(save_path))","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:54.487962Z","iopub.execute_input":"2024-03-02T00:18:54.488288Z","iopub.status.idle":"2024-03-02T00:18:54.494306Z","shell.execute_reply.started":"2024-03-02T00:18:54.488258Z","shell.execute_reply":"2024-03-02T00:18:54.493341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion,device):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n         for batch in iterator:\n            x, y_in, y_out = batch\n            y_in,x, y_out = y_in.to(device), x.to(device), y_out.to(device)\n            audio_features = model.embed_audio(x)  \n            logits = model.logits(y_in, audio_features=audio_features)\n            loss = loses(logits.view(-1, logits.size(-1)), y_out.view(-1))\n            epoch_loss += loss.item()\n    return epoch_loss / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:54.495665Z","iopub.execute_input":"2024-03-02T00:18:54.496418Z","iopub.status.idle":"2024-03-02T00:18:54.508742Z","shell.execute_reply.started":"2024-03-02T00:18:54.496383Z","shell.execute_reply":"2024-03-02T00:18:54.507922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef train_step(model,train_iter,optimizer: torch.optim.Optimizer,accum_grad_steps: int,train_only_decoder: bool,loses,device):\n    model.train()\n    total_loss = 0\n    for _ in range(accum_grad_steps):\n        x, y_in, y_out = next(train_iter)\n        y_in,x, y_out = y_in.to(device), x.to(device), y_out.to(device)\n        if train_only_decoder:\n            with torch.no_grad():\n                audio_features = model.embed_audio(x)\n        else:\n            audio_features = model.embed_audio(x)        \n        \n        logits = model.logits(y_in, audio_features=audio_features)\n        #loss = F.cross_entropy(logits.transpose(1, 2), y_out)\n        #print(logits.shape)\n        #print(logits.view(-1, logits.shape[-1]).shape)\n        #print( logits.shape[-1])\n        loss = loses(logits.view(-1, logits.size(-1)), y_out.view(-1))\n        #print(logits.view(-1, tokenizer.encoding.n_vocab).shape)\n        #loss = F.cross_entropy(logits.view(-1, tokenizer.encoding.n_vocab), y_out)\n        loss = loss / accum_grad_steps\n        loss.backward()\n        total_loss += loss.item()\n\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n    optimizer.step()\n    optimizer.zero_grad()\n\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:54.510817Z","iopub.execute_input":"2024-03-02T00:18:54.51115Z","iopub.status.idle":"2024-03-02T00:18:54.520813Z","shell.execute_reply.started":"2024-03-02T00:18:54.511119Z","shell.execute_reply":"2024-03-02T00:18:54.519936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer =  torch.optim.AdamW(model.parameters(), lr=1e-5)\nloses = torch.nn.CrossEntropyLoss(ignore_index=-100).to(device)\n#loses = torch.nn.CrossEntropyLoss(ignore_index=-100)\ndef infinite_iter(data_loader: DataLoader):\n    while True:\n        for batch in data_loader:\n            yield batch\nfrom tqdm import tqdm\npbar = tqdm(range(1, 91))\ntrain_iter = infinite_iter(DataLoaders)\nfor step in pbar:\n    train_loss =train_step(model,train_iter,optimizer,64,False,loses,device)\n    val_loss =evaluate(model,DataLoaders_val,loses,device)\n    pbar.set_postfix({\"loss\": train_loss,\"val loss\": val_loss})\n    print(\"step -> \", step,\" train_loss = \", train_loss,\" val_loss = \", val_loss)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T00:18:54.521715Z","iopub.execute_input":"2024-03-02T00:18:54.522018Z","iopub.status.idle":"2024-03-02T00:21:17.489499Z","shell.execute_reply.started":"2024-03-02T00:18:54.521989Z","shell.execute_reply":"2024-03-02T00:21:17.488192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\ndef remove_path_and_contents(path):\n    try:\n        # Remove the entire path and its contents\n        shutil.rmtree(path)\n        print(f\"Removed path and its contents: {path}\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Replace 'your_path_here' with the actual path you want to remove\npath_to_remove = '/kaggle/working/LibriStutter_Data'\nremove_path_and_contents(path_to_remove)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the saving path\nsave_pasth = \"/kaggle/working/model3.pt\"  # Replace with your desired path\n\n# Save the model state dictionary\ntorch.save(model.state_dict(), save_pasth)\n\nprint(f\"Model saved successfully to: {save_pasth}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nmodels = Whispersa(dimensions_instance).to(device)\nsave_path = \"/kaggle/input/speech-pro1/model.pt\"  # Replace with your desired path\nmodels.load_state_dict(torch.load(save_path))\n# Define the saving path\nsave_pasth = \"/kaggle/working/model.pt\"  # Replace with your desired path\n\n# Save the model state dictionary\ntorch.save(models.state_dict(), save_pasth)\n\nprint(f\"Model saved successfully to: {save_pasth}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nmodelsa = Whispersa(dimensions_instance).to(device)\nsave_path = \"/kaggle/input/speech-pro1/model1.pt\"  # Replace with your desired path\nmodelsa.load_state_dict(torch.load(save_path))\n# Define the saving path\nsave_pasth = \"/kaggle/working/model1.pt\"  # Replace with your desired path\n\n# Save the model state dictionary\ntorch.save(modelsa.state_dict(), save_pasth)\n\nprint(f\"Model saved successfully to: {save_pasth}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nmodelsaa = Whispersa(dimensions_instance).to(device)\nsave_path = \"/kaggle/input/speech-pro1/model2.pt\"  # Replace with your desired path\nmodelsaa.load_state_dict(torch.load(save_path))\n# Define the saving path\nsave_pasth = \"/kaggle/working/model2.pt\"  # Replace with your desired path\n\n# Save the model state dictionary\ntorch.save(modelsaa.state_dict(), save_pasth)\n\nprint(f\"Model saved successfully to: {save_pasth}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport os\nos.chdir(\"/kaggle/input/whispersa\")\nfrom model import (Whispersa,TextDecoder,AudioEncoder,ModelDimensions)\n!pip install tiktoken\n!pip install more_itertools\nimport os\nos.chdir(\"/kaggle/input/tokenizer\")\nfrom tokenizer import Tokenizer, get_tokenizer\ntokenizer =  get_tokenizer()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T21:16:36.162488Z","iopub.execute_input":"2024-03-02T21:16:36.162875Z","iopub.status.idle":"2024-03-02T21:17:06.541221Z","shell.execute_reply.started":"2024-03-02T21:16:36.162815Z","shell.execute_reply":"2024-03-02T21:17:06.540084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Create an instance of ModelDimensions\ndimensions_instance = ModelDimensions(\n       n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=tokenizer.encoding.n_vocab, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n\n)\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\"cpu\"\nmodel = Whispersa(dimensions_instance).to(device)\nsave_path = \"/kaggle/input/speech-pro1/model1.pt\"  # Replace with your desired path\nmodel.load_state_dict(torch.load(save_path))\nmodel.to(device)'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T21:17:06.543267Z","iopub.execute_input":"2024-03-02T21:17:06.543831Z","iopub.status.idle":"2024-03-02T21:17:16.172048Z","shell.execute_reply.started":"2024-03-02T21:17:06.543788Z","shell.execute_reply":"2024-03-02T21:17:16.171081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model.device'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport os\nos.chdir(\"/kaggle/input/transcribe\")\nfrom transcribe import transcribe'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T21:17:22.558727Z","iopub.execute_input":"2024-03-02T21:17:22.559136Z","iopub.status.idle":"2024-03-02T21:17:22.61748Z","shell.execute_reply.started":"2024-03-02T21:17:22.559103Z","shell.execute_reply":"2024-03-02T21:17:22.616428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nresult = transcribe(model, '/kaggle/input/libri-stutter/LibriStutter Audio/445/123857/445-123857-0000.flac', language='en', temperature=1.0, word_timestamps=True)\nprint(result['text'])\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-02T21:17:22.904991Z","iopub.execute_input":"2024-03-02T21:17:22.905565Z","iopub.status.idle":"2024-03-02T21:17:34.433681Z","shell.execute_reply.started":"2024-03-02T21:17:22.905536Z","shell.execute_reply":"2024-03-02T21:17:34.432462Z"},"trusted":true},"execution_count":null,"outputs":[]}]}